# Character Expression Mapping Guide

This guide explains how to use the audio-to-expression data generated by the OVRLipSync script to animate your character's facial expressions.

## Generated Files

The script generates two output formats:

1. **JSON File** (`*_expressions.json`) - For programmatic access
2. **CSV File** (`*_expressions.csv`) - For animation software import

## Viseme Data Structure

### 15 OVRLipSync Visemes

Each frame contains weights (0.0 to 1.0) for these 15 visemes:

| Viseme | Description | Mouth Shape | Common Sounds |
|--------|-------------|-------------|---------------|
| `sil` | Silence | Mouth closed/neutral | Silence, pauses |
| `PP` | Lips together | Pressed lips | P, B, M sounds |
| `FF` | Lower lip to upper teeth | Lip bite | F, V sounds |
| `TH` | Tongue between teeth | Tongue out | TH sounds |
| `DD` | Tongue to roof of mouth | Tongue up | D, T, N, L sounds |
| `kk` | Back of tongue to soft palate | Tongue back | K, G sounds |
| `CH` | Tongue and lips forward | Pursed + tongue | CH, SH, J sounds |
| `SS` | Tongue near roof, air flow | Slight opening | S, Z sounds |
| `nn` | Tongue to roof, nasal | Nasal position | N, NG sounds |
| `RR` | Tongue pulled back | Rolled tongue | R sounds |
| `aa` | Open mouth | Wide open | AH sounds |
| `E` | Slightly open | Small opening | EH, AE sounds |
| `I` | Small opening | Narrow opening | IH, EY sounds |
| `O` | Rounded lips | Round opening | OH, OW sounds |
| `U` | Tight rounded lips | Small round | UH, UW sounds |

### Additional Data

- **Timestamp**: Time in seconds for frame synchronization
- **Dominant Viseme**: The strongest viseme for this frame
- **Laugh Score**: Detected laughter intensity (0.0 to 1.0)

## Mapping to Character Rigs

### Blender Integration

```python
import json
import bpy

# Load the expression data
with open('voice_expressions.json', 'r') as f:
    data = json.load(f)

# Get your character's armature
armature = bpy.context.object
pose_bones = armature.pose.bones

# Map visemes to shape keys or bone rotations
viseme_mapping = {
    'sil': 'mouth_closed',
    'aa': 'mouth_open',
    'PP': 'mouth_lips_together',
    'O': 'mouth_round',
    # ... add your character's shape key names
}

# Animate over frames
for i, frame_data in enumerate(data['frames']):
    frame_num = int(frame_data['timestamp'] * 24)  # 24 fps
    bpy.context.scene.frame_set(frame_num)
    
    # Apply viseme weights
    for viseme, weight in frame_data['visemes'].items():
        if viseme in viseme_mapping:
            shape_key_name = viseme_mapping[viseme]
            if shape_key_name in bpy.context.object.data.shape_keys.key_blocks:
                bpy.context.object.data.shape_keys.key_blocks[shape_key_name].value = weight
                bpy.context.object.data.shape_keys.key_blocks[shape_key_name].keyframe_insert("value")
```

### Unity Integration

```csharp
using UnityEngine;
using System.Collections.Generic;

[System.Serializable]
public class VisemeFrame
{
    public float timestamp;
    public Dictionary<string, float> visemes;
    public string dominant_viseme;
    public float laugh_score;
}

[System.Serializable]
public class ExpressionData
{
    public List<VisemeFrame> frames;
}

public class CharacterExpressionController : MonoBehaviour
{
    [SerializeField] private SkinnedMeshRenderer faceMesh;
    private ExpressionData expressionData;
    private float startTime;
    
    // Map visemes to blend shape indices
    private Dictionary<string, int> visemeToBlendShape = new Dictionary<string, int>
    {
        {"sil", 0},    // Mouth_Closed
        {"aa", 1},     // Mouth_Open
        {"PP", 2},     // Mouth_Lips_Together
        {"O", 3},      // Mouth_Round
        // ... map to your character's blend shapes
    };
    
    void Start()
    {
        // Load expression data from JSON
        string jsonContent = Resources.Load<TextAsset>("voice_expressions").text;
        expressionData = JsonUtility.FromJson<ExpressionData>(jsonContent);
        startTime = Time.time;
    }
    
    void Update()
    {
        float currentTime = Time.time - startTime;
        
        // Find current frame
        VisemeFrame currentFrame = FindFrameAtTime(currentTime);
        if (currentFrame != null)
        {
            ApplyVisemes(currentFrame);
        }
    }
    
    void ApplyVisemes(VisemeFrame frame)
    {
        // Reset all blend shapes
        for (int i = 0; i < faceMesh.sharedMesh.blendShapeCount; i++)
        {
            faceMesh.SetBlendShapeWeight(i, 0f);
        }
        
        // Apply viseme weights
        foreach (var viseme in frame.visemes)
        {
            if (visemeToBlendShape.ContainsKey(viseme.Key))
            {
                int blendShapeIndex = visemeToBlendShape[viseme.Key];
                faceMesh.SetBlendShapeWeight(blendShapeIndex, viseme.Value * 100f);
            }
        }
        
        // Handle laughter
        if (frame.laugh_score > 0.5f)
        {
            // Trigger laugh animation or expression
            ApplyLaughExpression(frame.laugh_score);
        }
    }
}
```

### Maya/3ds Max Integration

Import the CSV file directly:

1. **Maya**: Use `File > Import` and select the CSV file
2. **3ds Max**: Use `Animation > Import Animation` 
3. Map columns to your character's facial controls
4. Set keyframes based on timestamps

## Character Rig Requirements

### Recommended Facial Controls

Your character should have controls for:

**Basic Mouth Shapes:**
- Mouth open/close
- Lip compression
- Lip rounding
- Tongue visibility

**Advanced Controls:**
- Individual viseme shapes
- Lip corner controls
- Jaw open/close
- Tongue position

### Blend Shape Setup

Create blend shapes that correspond to the 15 visemes. You can combine multiple visemes for more natural transitions:

```
mouth_neutral (sil)
mouth_open_wide (aa)
mouth_lips_pressed (PP)
mouth_lip_bite (FF)
mouth_tongue_out (TH)
mouth_tongue_up (DD)
mouth_back_tongue (kk)
mouth_forward_pucker (CH)
mouth_slight_open (SS)
mouth_nasal (nn)
mouth_r_shape (RR)
mouth_e_shape (E)
mouth_i_shape (I)
mouth_o_round (O)
mouth_u_tight (U)
```

## Performance Tips

1. **Frame Rate**: Match your animation frame rate to the audio frame rate
2. **Smoothing**: Apply smoothing to reduce jitter between frames
3. **Weighting**: You may need to scale viseme weights (multiply by factors like 0.7-1.3)
4. **Blending**: Use interpolation between frames for smoother animation
5. **Dominant Only**: For performance, you can use only the dominant viseme per frame

## Laugh Detection

The `laugh_score` can be used to:
- Trigger additional facial expressions (smile, eye crinkles)
- Add head movement or body animation
- Modify breathing patterns
- Enhance emotional expression

## Example Usage Patterns

### Simple Implementation (Dominant Viseme Only)
```python
# Use only the strongest viseme per frame
for frame in frames:
    dominant = frame['dominant_viseme']
    apply_viseme(dominant, intensity=1.0)
```

### Advanced Implementation (Weighted Blending)
```python
# Blend multiple visemes based on weights
for frame in frames:
    for viseme, weight in frame['visemes'].items():
        if weight > 0.1:  # Threshold to reduce noise
            apply_viseme(viseme, intensity=weight)
```

### Performance Implementation (Keyframe Reduction)
```python
# Only set keyframes when visemes change significantly
last_dominant = None
for frame in frames:
    if frame['dominant_viseme'] != last_dominant:
        set_keyframe(frame['timestamp'], frame['dominant_viseme'])
        last_dominant = frame['dominant_viseme']
```

This data provides a solid foundation for realistic lip-sync and facial animation that responds naturally to speech patterns and emotional content in your audio.
